{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ee48e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "import ecole\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from  acr_bb import Observation, ACRBBenv, DefaultBranchingPolicy, RandomPolicy\n",
    "\n",
    "\n",
    "MAX_SAMPLES = 10000\n",
    "\n",
    "N = 8 # antennas\n",
    "M = 4 # users\n",
    "expert_prob = 0.5\n",
    "\n",
    "def instance_generator(M, N):\n",
    "    while 1:\n",
    "        yield np.random.randn(2,N,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1b3b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instances = np.random.randn(MAX_SAMPLES, 2, N, M)\n",
    "instances = instance_generator(M,N)\n",
    "\n",
    "env = ACRBBenv()\n",
    "\n",
    "expert_policy = DefaultBranchingPolicy()\n",
    "random_policy = RandomPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b39262e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, 208 samples collected so far\n",
      "Episode 2, 546 samples collected so far\n",
      "Episode 3, 651 samples collected so far\n",
      "Episode 4, 802 samples collected so far\n",
      "Episode 5, 1179 samples collected so far\n",
      "Episode 6, 1406 samples collected so far\n",
      "Episode 7, 1520 samples collected so far\n",
      "Episode 8, 1659 samples collected so far\n",
      "Episode 9, 1749 samples collected so far\n",
      "Episode 10, 1961 samples collected so far\n",
      "Episode 11, 2109 samples collected so far\n",
      "Episode 12, 2313 samples collected so far\n",
      "Episode 13, 2456 samples collected so far\n",
      "Episode 14, 2644 samples collected so far\n",
      "Episode 15, 2822 samples collected so far\n",
      "Episode 16, 3021 samples collected so far\n",
      "Episode 17, 3272 samples collected so far\n",
      "Episode 18, 3619 samples collected so far\n",
      "Episode 19, 3827 samples collected so far\n",
      "Episode 20, 3975 samples collected so far\n",
      "Episode 21, 4135 samples collected so far\n",
      "Episode 22, 4351 samples collected so far\n",
      "Episode 23, 4737 samples collected so far\n",
      "Episode 24, 4946 samples collected so far\n",
      "Episode 25, 5088 samples collected so far\n",
      "Episode 26, 5225 samples collected so far\n",
      "Episode 27, 5430 samples collected so far\n",
      "Episode 28, 5547 samples collected so far\n",
      "Episode 29, 5882 samples collected so far\n",
      "Episode 30, 6061 samples collected so far\n",
      "Episode 31, 6339 samples collected so far\n",
      "Episode 32, 6515 samples collected so far\n",
      "Episode 33, 6715 samples collected so far\n",
      "Episode 34, 6950 samples collected so far\n",
      "Episode 35, 7047 samples collected so far\n",
      "Episode 36, 7249 samples collected so far\n",
      "Episode 37, 7892 samples collected so far\n",
      "Episode 38, 8000 samples collected so far\n",
      "Episode 39, 8302 samples collected so far\n",
      "Episode 40, 8383 samples collected so far\n",
      "Episode 41, 8472 samples collected so far\n",
      "Episode 42, 8528 samples collected so far\n",
      "Episode 43, 8955 samples collected so far\n",
      "Episode 44, 9061 samples collected so far\n",
      "Episode 45, 9202 samples collected so far\n",
      "Episode 46, 9405 samples collected so far\n",
      "Episode 47, 9622 samples collected so far\n",
      "Episode 48, 9706 samples collected so far\n",
      "Episode 49, 9767 samples collected so far\n",
      "Episode 50, 10000 samples collected so far\n"
     ]
    }
   ],
   "source": [
    "episode_counter, sample_counter = 0, 0\n",
    "Path('samples/').mkdir(exist_ok=True)\n",
    "\n",
    "# We will solve problems (run episodes) until we have saved enough samples\n",
    "max_samples_reached = False\n",
    "while not max_samples_reached:\n",
    "    episode_counter += 1\n",
    "    \n",
    "    observation, action_set, _, done, _ = env.reset(next(instances))\n",
    "    while not done:\n",
    "        if np.random.rand(1) > expert_prob:\n",
    "            action_id = expert_policy.select_variable(observation, action_set)\n",
    "            expert = True\n",
    "        else:\n",
    "            action_id = random_policy.select_variable(observation, action_set)\n",
    "            expert = False\n",
    "            \n",
    "        # Only save samples if they are coming from the expert (strong branching)\n",
    "        if expert and not max_samples_reached:\n",
    "            sample_counter += 1\n",
    "            data = [observation, action_id, action_set]\n",
    "            filename = f'samples/sample_{sample_counter}.pkl'\n",
    "\n",
    "            with gzip.open(filename, 'wb') as f:\n",
    "                pickle.dump(data, f)\n",
    "            \n",
    "            # If we collected enough samples, we finish the current episode but stop saving samples\n",
    "            if sample_counter == MAX_SAMPLES:\n",
    "                max_samples_reached = True\n",
    "\n",
    "        observation, action_set, _, done, _ = env.step(action_id)\n",
    "\n",
    "        \n",
    "    print(f\"Episode {episode_counter}, {sample_counter} samples collected so far\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "982bd33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "NB_EPOCHS = 50\n",
    "PATIENCE = 10\n",
    "EARLY_STOPPING = 20\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2d461350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "NB_EPOCHS = 50\n",
    "PATIENCE = 10\n",
    "EARLY_STOPPING = 20\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class BipartiteNodeData(torch_geometric.data.Data):\n",
    "    \"\"\"\n",
    "    This class encode a node bipartite graph observation as returned by the `ecole.observation.NodeBipartite` \n",
    "    observation function in a format understood by the pytorch geometric data handlers.\n",
    "    \"\"\"\n",
    "    def __init__(self, antenna_features, edge_indices, edge_features, variable_features,\n",
    "                 candidates, candidate_choice):\n",
    "        super().__init__()\n",
    "        if antenna_features is not None:\n",
    "            self.antenna_features = torch.FloatTensor(antenna_features)\n",
    "            self.edge_index = torch.LongTensor(edge_indices.astype(np.int64))\n",
    "            self.edge_attr = torch.FloatTensor(edge_features)\n",
    "            self.variable_features = torch.FloatTensor(variable_features)\n",
    "            self.candidates = candidates\n",
    "            self.nb_candidates = len(candidates)\n",
    "            self.candidate_choices = candidate_choice\n",
    "\n",
    "    def __inc__(self, key, value, *ags, **kwargs):\n",
    "        \"\"\"\n",
    "        We overload the pytorch geometric method that tells how to increment indices when concatenating graphs \n",
    "        for those entries (edge index, candidates) for which this is not obvious.\n",
    "        \"\"\"\n",
    "        if key == 'edge_index':\n",
    "            return torch.tensor([[self.antenna_features.size(0)], [self.variable_features.size(0)]])\n",
    "        elif key == 'candidates':\n",
    "            return self.variable_features.size(0)\n",
    "        else:\n",
    "            return super().__inc__(key, value)\n",
    "\n",
    "\n",
    "class GraphDataset(torch_geometric.data.Dataset):\n",
    "    \"\"\"\n",
    "    This class encodes a collection of graphs, as well as a method to load such graphs from the disk.\n",
    "    It can be used in turn by the data loaders provided by pytorch geometric.\n",
    "    \"\"\"\n",
    "    def __init__(self, sample_files):\n",
    "        super().__init__(root=None, transform=None, pre_transform=None)\n",
    "        self.sample_files = sample_files\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.sample_files)\n",
    "\n",
    "    def get(self, index):\n",
    "        \"\"\"\n",
    "        This method loads a node bipartite graph observation as saved on the disk during data collection.\n",
    "        \"\"\"\n",
    "        with gzip.open(self.sample_files[index], 'rb') as f:\n",
    "            sample = pickle.load(f)\n",
    "\n",
    "        sample_observation, sample_action_id, sample_action_set = sample\n",
    "        \n",
    "        # We note on which variables we were allowed to branch, the scores as well as the choice \n",
    "        # taken by expert branching (relative to the candidates)\n",
    "        candidates = torch.LongTensor(np.array(sample_action_set, dtype=np.int32))\n",
    "#         candidate_choice = sample_action_id\n",
    "        candidate_choice = torch.where(candidates == sample_action_id)[0][0]\n",
    "\n",
    "        graph = BipartiteNodeData(sample_observation.antenna_features, sample_observation.edge_index, \n",
    "                                  sample_observation.edge_features, sample_observation.variable_features,\n",
    "                                  candidates, candidate_choice)\n",
    "        \n",
    "        # We must tell pytorch geometric how many nodes there are, for indexing purposes\n",
    "        graph.num_nodes = sample_observation.antenna_features.shape[0] + sample_observation.variable_features.shape[0]\n",
    "        \n",
    "        return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5ff48a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = torch.LongTensor(np.arange(1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "74a90de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_files = [str(path) for path in Path('samples/').glob('sample_*.pkl')]\n",
    "train_files = sample_files[:int(0.8*len(sample_files))]\n",
    "valid_files = sample_files[int(0.8*len(sample_files)):]\n",
    "\n",
    "train_data = GraphDataset(train_files)\n",
    "train_loader = torch_geometric.data.DataLoader(train_data, batch_size=5, shuffle=True)\n",
    "valid_data = GraphDataset(valid_files)\n",
    "valid_loader = torch_geometric.data.DataLoader(valid_data, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4fc834a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNPolicy(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        emb_size = 64\n",
    "        antenna_nfeats = 3\n",
    "        edge_nfeats = 3\n",
    "        var_nfeats = 9\n",
    "\n",
    "        # CONSTRAINT EMBEDDING\n",
    "        self.antenna_embedding = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(antenna_nfeats),\n",
    "            torch.nn.Linear(antenna_nfeats, emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(emb_size, emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # EDGE EMBEDDING\n",
    "        self.edge_embedding = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(edge_nfeats),\n",
    "            torch.nn.Linear(edge_nfeats, emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(emb_size, emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "            \n",
    "        )\n",
    "\n",
    "        # VARIABLE EMBEDDING\n",
    "        self.var_embedding = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(var_nfeats),\n",
    "            torch.nn.Linear(var_nfeats, emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(emb_size, emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv_v_to_c = BipartiteGraphConvolution()\n",
    "        self.conv_c_to_v = BipartiteGraphConvolution()\n",
    "\n",
    "        self.output_module = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_size, emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(emb_size, 1, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, constraint_features, edge_indices, edge_features, variable_features):\n",
    "        reversed_edge_indices = torch.stack([edge_indices[1], edge_indices[0]], dim=0)\n",
    "        \n",
    "        # First step: linear embedding layers to a common dimension (64)\n",
    "        constraint_features = self.antenna_embedding(constraint_features)\n",
    "        edge_features = self.edge_embedding(edge_features)\n",
    "        variable_features = self.var_embedding(variable_features)\n",
    "        \n",
    "\n",
    "        # Two half convolutions\n",
    "#         print('var', variable_features.shape, 'cons', constraint_features.shape, 'edge', reversed_edge_indices.shape, 'edge_f', edge_features.shape)\n",
    "        constraint_features = self.conv_v_to_c(variable_features, reversed_edge_indices, edge_features, constraint_features)\n",
    "        variable_features = self.conv_c_to_v(constraint_features, edge_indices, edge_features, variable_features)\n",
    "\n",
    "        # A final MLP on the variable features\n",
    "        output = self.output_module(variable_features).squeeze(-1)\n",
    "        return output\n",
    "    \n",
    "\n",
    "class BipartiteGraphConvolution(torch_geometric.nn.MessagePassing):\n",
    "    \"\"\"\n",
    "    The bipartite graph convolution is already provided by pytorch geometric and we merely need \n",
    "    to provide the exact form of the messages being passed.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__('add')\n",
    "        emb_size = 64\n",
    "        \n",
    "        self.feature_module_left = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_size, emb_size)\n",
    "        )\n",
    "        self.feature_module_edge = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_size, emb_size, bias=False)\n",
    "        )\n",
    "        self.feature_module_right = torch.nn.Sequential(\n",
    "            torch.nn.Linear(emb_size, emb_size, bias=False)\n",
    "        )\n",
    "        self.feature_module_final = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(emb_size, emb_size)\n",
    "        )\n",
    "        \n",
    "        self.post_conv_module = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm(emb_size)\n",
    "        )\n",
    "\n",
    "        # output_layers\n",
    "        self.output_module = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2*emb_size, emb_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(emb_size, emb_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, left_features, edge_indices, edge_features, right_features):\n",
    "        \"\"\"\n",
    "        This method sends the messages, computed in the message method.\n",
    "        \"\"\"\n",
    "        output = self.propagate(edge_indices, size=(left_features.shape[0], right_features.shape[0]), \n",
    "                                node_features=(left_features, right_features), edge_features=edge_features)\n",
    "        return self.output_module(torch.cat([self.post_conv_module(output), right_features], dim=-1))\n",
    "\n",
    "    def message(self, node_features_i, node_features_j, edge_features):\n",
    "        output = self.feature_module_final(self.feature_module_left(node_features_i) \n",
    "                                           + self.feature_module_edge(edge_features) \n",
    "                                           + self.feature_module_right(node_features_j))\n",
    "        return output\n",
    "    \n",
    "\n",
    "policy = GNNPolicy().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b84b4e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▎                                                                                                                         | 199/1600 [00:01<00:11, 126.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.832, accuracy 0.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:01<00:00, 265.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.721, accuracy 0.667\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▍                                                                                                                          | 199/1600 [00:02<00:20, 69.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.682, accuracy 0.720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:01<00:00, 298.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.588, accuracy 0.755\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▎                                                                                                                         | 199/1600 [00:01<00:11, 122.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.643, accuracy 0.732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:02<00:00, 159.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.544, accuracy 0.766\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▎                                                                                                                         | 199/1600 [00:01<00:11, 119.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.569, accuracy 0.778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:01<00:00, 307.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.557, accuracy 0.758\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▍                                                                                                                          | 199/1600 [00:03<00:22, 63.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.575, accuracy 0.776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:01<00:00, 311.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.477, accuracy 0.806\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▍                                                                                                                          | 199/1600 [00:03<00:21, 66.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.543, accuracy 0.792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:01<00:00, 293.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.519, accuracy 0.786\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▍                                                                                                                          | 199/1600 [00:02<00:14, 97.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.520, accuracy 0.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:02<00:00, 159.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.542, accuracy 0.753\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▎                                                                                                                         | 199/1600 [00:01<00:12, 111.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.521, accuracy 0.794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:02<00:00, 195.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.549, accuracy 0.752\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▍                                                                                                                          | 199/1600 [00:02<00:17, 79.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.508, accuracy 0.799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:01<00:00, 331.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.503, accuracy 0.795\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▍                                                                                                                          | 199/1600 [00:02<00:17, 78.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.495, accuracy 0.800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:01<00:00, 208.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.514, accuracy 0.793\n",
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▎                                                                                                                         | 199/1600 [00:01<00:13, 101.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.439, accuracy 0.798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:02<00:00, 135.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.470, accuracy 0.802\n",
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▍                                                                                                                          | 199/1600 [00:02<00:16, 86.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.468, accuracy 0.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:02<00:00, 158.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.458, accuracy 0.804\n",
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▎                                                                                                                         | 199/1600 [00:01<00:12, 115.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.449, accuracy 0.825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:01<00:00, 200.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.420, accuracy 0.829\n",
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▍                                                                                                                          | 199/1600 [00:02<00:17, 78.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.484, accuracy 0.796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:01<00:00, 323.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.425, accuracy 0.844\n",
      "Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▍                                                                                                                          | 199/1600 [00:02<00:19, 73.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.502, accuracy 0.789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:01<00:00, 217.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.565, accuracy 0.755\n",
      "Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▍                                                                                                                          | 199/1600 [00:02<00:18, 75.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.483, accuracy 0.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:01<00:00, 210.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.447, accuracy 0.823\n",
      "Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▍                                                                                                                          | 199/1600 [00:02<00:18, 74.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.472, accuracy 0.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:01<00:00, 211.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.451, accuracy 0.770\n",
      "Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▎                                                                                                                         | 199/1600 [00:01<00:11, 118.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.458, accuracy 0.820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:02<00:00, 160.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.415, accuracy 0.797\n",
      "Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▍                                                                                                                          | 199/1600 [00:02<00:14, 98.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.453, accuracy 0.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:01<00:00, 240.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.464, accuracy 0.823\n",
      "Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▍                                                                                                                          | 199/1600 [00:02<00:20, 67.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.432, accuracy 0.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:01<00:00, 247.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.438, accuracy 0.818\n",
      "Epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▍                                                                                                                          | 199/1600 [00:03<00:22, 61.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.435, accuracy 0.828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:01<00:00, 261.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.376, accuracy 0.840\n",
      "Epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▍                                                                                                                          | 199/1600 [00:02<00:20, 68.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.452, accuracy 0.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:01<00:00, 240.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.409, accuracy 0.835\n",
      "Epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▍                                                                                                                          | 199/1600 [00:02<00:18, 75.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.456, accuracy 0.802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:02<00:00, 165.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.392, accuracy 0.842\n",
      "Epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▍                                                                                                                          | 199/1600 [00:02<00:16, 83.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.437, accuracy 0.823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:02<00:00, 176.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.434, accuracy 0.806\n",
      "Epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▎                                                                                                                         | 199/1600 [00:01<00:11, 126.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.471, accuracy 0.811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:02<00:00, 175.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.383, accuracy 0.834\n",
      "Epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▍                                                                                                                          | 199/1600 [00:02<00:17, 82.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.436, accuracy 0.817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:01<00:00, 323.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.416, accuracy 0.817\n",
      "Epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▍                                                                                                                          | 199/1600 [00:03<00:24, 56.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.425, accuracy 0.829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:02<00:00, 182.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.404, accuracy 0.830\n",
      "Epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▍                                                                                                                          | 199/1600 [00:03<00:22, 62.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.416, accuracy 0.839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:01<00:00, 247.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.365, accuracy 0.852\n",
      "Epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▍                                                                                                                          | 199/1600 [00:02<00:16, 87.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.428, accuracy 0.821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:02<00:00, 168.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.481, accuracy 0.803\n",
      "Epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▍                                                                                                                          | 199/1600 [00:02<00:15, 93.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.443, accuracy 0.829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:03<00:00, 123.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.387, accuracy 0.834\n",
      "Epoch 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▍                                                                                                                          | 199/1600 [00:02<00:16, 84.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.441, accuracy 0.803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:03<00:00, 125.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.391, accuracy 0.838\n",
      "Epoch 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▍                                                                                                                          | 199/1600 [00:02<00:15, 93.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.409, accuracy 0.827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:02<00:00, 153.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.398, accuracy 0.816\n",
      "Epoch 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▍                                                                                                                          | 199/1600 [00:02<00:14, 96.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.420, accuracy 0.811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:02<00:00, 153.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.373, accuracy 0.850\n",
      "Epoch 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▎                                                                                                                         | 199/1600 [00:01<00:12, 111.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.440, accuracy 0.820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:02<00:00, 171.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.437, accuracy 0.823\n",
      "Epoch 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████████████▍                                                                                                                          | 199/1600 [00:02<00:15, 88.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.396, accuracy 0.836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 400/400 [00:01<00:00, 290.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss: 0.417, accuracy 0.810\n",
      "Epoch 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|███████████▌                                                                                                                                | 132/1600 [00:02<00:24, 59.91it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_64737/3577143759.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train loss: {train_loss:0.3f}, accuracy {train_acc:0.3f}\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_64737/3577143759.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(policy, data_loader, optimizer)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mbatch_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mbatch_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/combopt/venv/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/combopt/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/combopt/venv/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/combopt/venv/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/combopt/venv/lib/python3.8/site-packages/torch_geometric/loader/dataloader.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/combopt/venv/lib/python3.8/site-packages/torch_geometric/loader/dataloader.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0melem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHeteroData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             return Batch.from_data_list(batch, self.follow_batch,\n\u001b[0m\u001b[1;32m     20\u001b[0m                                         self.exclude_keys)\n\u001b[1;32m     21\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/combopt/venv/lib/python3.8/site-packages/torch_geometric/data/batch.py\u001b[0m in \u001b[0;36mfrom_data_list\u001b[0;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     61\u001b[0m         Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         batch, slice_dict, inc_dict = collate(\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mdata_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/combopt/venv/lib/python3.8/site-packages/torch_geometric/data/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0minc_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mincs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mslice_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0minc_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mincs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def process(policy, data_loader, optimizer=None):\n",
    "    \"\"\"\n",
    "    This function will process a whole epoch of training or validation, depending on whether an optimizer is provided.\n",
    "    \"\"\"\n",
    "    mean_loss = 0\n",
    "    mean_acc = 0\n",
    "    mean_acc_copy = 0\n",
    "    \n",
    "    acc_list = []\n",
    "    n_samples_processed = 0\n",
    "    batch_count = 0\n",
    "    with torch.set_grad_enabled(optimizer is not None):\n",
    "        for batch in tqdm(data_loader):\n",
    "            batch_count += 5\n",
    "            batch = batch.to(DEVICE)\n",
    "            # Compute the logits (i.e. pre-softmax activations) according to the policy on the concatenated graphs\n",
    "            logits = policy(batch.antenna_features, batch.edge_index, batch.edge_attr, batch.variable_features)\n",
    "            # Index the results by the candidates, and split and pad them\n",
    "            logits = pad_tensor(logits[batch.candidates], batch.nb_candidates)\n",
    "\n",
    "            # Compute the usual cross-entropy classification loss\n",
    "            loss = F.cross_entropy(logits, torch.LongTensor(batch.candidate_choices))\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            predicted_bestindex = logits.max(dim=-1, keepdims=True).indices\n",
    "            accuracy = sum(predicted_bestindex.reshape(-1) == batch.candidate_choices)\n",
    "#             accuracy = (true_scores.gather(-1, predicted_bestindex) == true_bestscore).float().mean().item()\n",
    "\n",
    "            mean_loss += loss.item() * batch.num_graphs\n",
    "            mean_acc += float(accuracy)\n",
    "        \n",
    "            mean_acc_copy +=float(accuracy)\n",
    "            if batch_count >= 1000 and optimizer is not None:\n",
    "                acc_list = mean_acc_copy/batch_count\n",
    "                batch_count=0\n",
    "                mean_acc_copy= 0\n",
    "                break\n",
    "                \n",
    "            n_samples_processed += batch.num_graphs\n",
    "\n",
    "    mean_loss /= n_samples_processed\n",
    "    mean_acc /= n_samples_processed\n",
    "    return mean_loss, mean_acc, acc_list\n",
    "\n",
    "\n",
    "def pad_tensor(input_, pad_sizes, pad_value=-1e8):\n",
    "    \"\"\"\n",
    "    This utility function splits a tensor and pads each split to make them all the same size, then stacks them.\n",
    "    \"\"\"\n",
    "    max_pad_size = pad_sizes.max()\n",
    "    output = input_.split(pad_sizes.cpu().numpy().tolist())\n",
    "    output = torch.stack([F.pad(slice_, (0, max_pad_size-slice_.size(0)), 'constant', pad_value)\n",
    "                          for slice_ in output], dim=0)\n",
    "    return output\n",
    "\n",
    "acc_list = []\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=LEARNING_RATE)\n",
    "for epoch in range(NB_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    train_loss, train_acc, _ = process(policy, train_loader, optimizer)\n",
    "    print(f\"Train loss: {train_loss:0.3f}, accuracy {train_acc:0.3f}\" )\n",
    "\n",
    "    valid_loss, valid_acc, _ = process(policy, valid_loader, None)\n",
    "    print(f\"Valid loss: {valid_loss:0.3f}, accuracy {valid_acc:0.3f}\" )\n",
    "\n",
    "    acc_list.append(valid_acc)\n",
    "torch.save(policy.state_dict(), 'trained_params.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "df9d0d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Episode(object):\n",
    "    def __init__(self):\n",
    "        self.reward_history = []\n",
    "        self.reward_history = []\n",
    "        self.loss_history = []\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "episode = Episode()\n",
    "\n",
    "\n",
    "def get_graph_from_obs(self, sample_observation, sample_action_set):\n",
    "       \n",
    "        sample_action_id = sample_action_set[0] # doen't matter won't be used\n",
    "        # We note on which variables we were allowed to branch, the scores as well as the choice \n",
    "        # taken by expert branching (relative to the candidates)\n",
    "        candidates = torch.LongTensor(np.array(sample_action_set, dtype=np.int32))\n",
    "        candidate_choice = torch.where(candidates == sample_action_id)[0][0]\n",
    "\n",
    "        graph = BipartiteNodeData(sample_observation.antenna_features, sample_observation.edge_index, \n",
    "                                  sample_observation.edge_features, sample_observation.variable_features,\n",
    "                                  candidates, candidate_choice)\n",
    "        \n",
    "        # We must tell pytorch geometric how many nodes there are, for indexing purposes\n",
    "        graph.num_nodes = sample_observation.antenna_features.shape[0] + sample_observation.variable_features.shape[0]\n",
    "        \n",
    "        return graph\n",
    "\n",
    "def select_action(policy_net, obs, action_set, temperature):\n",
    "    sf = nn.Softmax()\n",
    "    graph = get_graph_from_obs(obs, action_set)\n",
    "    logits = policy(graph.antenna_features, graph.edge_index, graph.edge_features, graph.variable_features)\n",
    "    prob = Categorical(sf(logits/temperature))\n",
    "    action_id = prob.sample()\n",
    "    return action_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f44ea52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64737/2584110547.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  b = a(torch.rand(5))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "a = nn.Softmax()\n",
    "b = a(torch.rand(5))\n",
    "m = Categorical(b)\n",
    "m.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e671b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49da0912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_episodes):\n",
    "    running_reward = -1\n",
    "    for i in range(num_episodes):\n",
    "        obs, action_set, reward, done, _ = env.reset()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b11d335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a66e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13007c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b10260b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d43f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4755118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42b65db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c62a67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95180004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746475c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707ca93b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
